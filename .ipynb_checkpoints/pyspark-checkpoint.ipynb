{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef509c49-993e-49c1-855d-77c3425c736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing pyspark library to run spark in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214c8f5d-307c-4317-847a-cd9490f34402",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "#we already install this library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a784d60-013a-409e-baf0-263dc00e182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3969b836-826f-4a2f-8202-065bcbee39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the library to start the session\n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02e6aea-00fd-4d86-acc6-08d5d32ab680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of spark\n",
    "spark_runing = SparkSession.builder.appName('testing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30aa48eb-76e7-4986-895d-cac405af4b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>testing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1fee1af2c10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_runing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3eeb64-bc87-4ba4-8a21-9f2dcab32b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Krish</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sudhanshu</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name  age  Experience  Salary\n",
       "0      Krish   31          10   30000\n",
       "1  Sudhanshu   30           8   25000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "demo_csv = pd.read_csv('test1.csv')\n",
    "demo_csv.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce8737-ec42-433f-807c-e63b3669be00",
   "metadata": {},
   "source": [
    "# read a dataset with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c2ad67e-fdb5-46c3-ad28-b18f712d3724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark = spark_runing.read.csv('test1.csv')\n",
    "#show a resume of what of its\n",
    "df_pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444b27f-5b56-4eb4-a982-601b82714753",
   "metadata": {},
   "source": [
    "With this command shows the whole table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f236ba-330e-4c99-9d44-3cc9fd0a2073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|      _c0|_c1|       _c2|   _c3|\n",
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show the data\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "208adaa0-a9c4-4115-9cdf-ebf192f6be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#une the title as title in the imported file\n",
    "#option() = alows to consider the 1st row as header\n",
    "\n",
    "df_pyspark_title = spark_runing.read.option('header','true').csv('test1.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c779b96-283b-434b-a82e-1729c94cfd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_title.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f97cc-e167-424c-a5d7-2835123ab8bb",
   "metadata": {},
   "source": [
    "### type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "098770c0-478f-4cae-a1f2-1a83bb47e6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifying the type in spark\n",
    "type(df_pyspark_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae64afb1-b0d3-43d2-ac56-13a135f15026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifying the type in pandas\n",
    "type(demo_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea90dff5-6650-4705-965d-30e537c7a9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#description of the dataset\n",
    "#this prin only strings cause we dont assign a type when importing\n",
    "df_pyspark_title.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be7ecbe-f37b-42b2-8267-46eb7f548135",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e577d44f-9cda-4231-8b45-2fcc1f738d3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pyspark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40df15-fd89-423c-ad90-41aa4cc3a3d7",
   "metadata": {},
   "source": [
    "* PySpark Dataframe\n",
    "* Reading The Dataset\n",
    "* Checking the Datatypes of the Column(Schema)\n",
    "* Selecting Columns And Indexing\n",
    "* Check Describe option similar to Pandas\n",
    "* Adding Columns\n",
    "* Dropping columns\n",
    "* Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d793963-515c-4d8a-bf61-9d54e675bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inferSchema = allows to got the datatype from thefile \n",
    "df_pyspark_title_inferSchema = spark_runing.read.option('header','true').csv('test1.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8404a426-6c10-455c-b0ba-3ede516c4a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_title_inferSchema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02349d1c-ae99-4f51-8a13-5a80049a4a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns all column names as a list.\n",
    "df_pyspark_title_inferSchema.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e68a671-04f0-4270-bd63-0b6873f2963a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', age=31, Experience=10, Salary=30000),\n",
       " Row(Name='Sudhanshu', age=30, Experience=8, Salary=25000)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns the first ``n`` rows.\n",
    "df_pyspark_title_inferSchema.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed25781-7ee5-4bcf-a8a1-6b1ff2c2fce5",
   "metadata": {},
   "source": [
    "# selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bf5a37f-a97f-4273-bb0b-e513192270f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|    Krish|\n",
      "|Sudhanshu|\n",
      "|    Sunny|\n",
      "|     Paul|\n",
      "|   Harsha|\n",
      "|  Shubham|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selecting one column\n",
    "df_pyspark_title_inferSchema.select('name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f6cd3-73b0-4215-be58-70f492b613ce",
   "metadata": {},
   "source": [
    "selecting 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1268d17c-1817-4df5-8119-32bab9f62b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     name|salary|\n",
      "+---------+------+\n",
      "|    Krish| 30000|\n",
      "|Sudhanshu| 25000|\n",
      "|    Sunny| 20000|\n",
      "|     Paul| 20000|\n",
      "|   Harsha| 15000|\n",
      "|  Shubham| 18000|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_title_inferSchema.select(['name','salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5539a-6b2d-48d9-8c7b-ecdd3f78acaf",
   "metadata": {},
   "source": [
    "cheking the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41353104-153b-4d6f-9d89-8370a3ee854a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('age', 'int'), ('Experience', 'int'), ('Salary', 'int')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the data types\n",
    "df_pyspark_title_inferSchema.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d60ffed-daa4-43ef-89a2-d2b028237b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-----------------+------------------+\n",
      "|summary|  Name|               age|       Experience|            Salary|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "|  count|     6|                 6|                6|                 6|\n",
      "|   mean|  null|26.333333333333332|4.666666666666667|21333.333333333332|\n",
      "| stddev|  null| 4.179314138308661|3.559026084010437| 5354.126134736337|\n",
      "|    min|Harsha|                21|                1|             15000|\n",
      "|    max| Sunny|                31|               10|             30000|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show the description in stadistics\n",
    "df_pyspark_title_inferSchema.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a4d9d3-c70b-43b9-8152-2f575cbf5fe8",
   "metadata": {},
   "source": [
    "# adding column in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d5dca10-6af2-434d-b83e-63dfbe90ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_add_column = df_pyspark_title_inferSchema.withColumn('experience after 2 years', df_pyspark_title_inferSchema['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c777afc-ee25-42b7-92cb-0bee806057fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+------------------------+\n",
      "|     Name|age|Experience|Salary|experience after 2 years|\n",
      "+---------+---+----------+------+------------------------+\n",
      "|    Krish| 31|        10| 30000|                      12|\n",
      "|Sudhanshu| 30|         8| 25000|                      10|\n",
      "|    Sunny| 29|         4| 20000|                       6|\n",
      "|     Paul| 24|         3| 20000|                       5|\n",
      "|   Harsha| 21|         1| 15000|                       3|\n",
      "|  Shubham| 23|         2| 18000|                       4|\n",
      "+---------+---+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_add_column.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1c7c5-0188-4a4c-a2fb-43bcf45cb999",
   "metadata": {},
   "source": [
    "# dropping the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f56b11-2ef1-41db-84c5-0dd99e6cfafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark2 = df_pyspark_add_column .drop('experience after 2 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "344b2930-1b9a-4236-bb68-aac4142b11fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0897a9-d8e2-4813-bc74-75b1231c3074",
   "metadata": {},
   "source": [
    "# rename column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6a9ce81-5845-47b1-aa56-638a3e31af72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|      xxx|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.withColumnRenamed('name', 'xxx').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a360dc6-db26-4f2e-b31d-08034c867232",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3e561-c380-462e-88c3-e433b30e2af9",
   "metadata": {},
   "source": [
    "# Pyspark Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c948e75-f2c6-4455-b8f8-bb177a168aea",
   "metadata": {},
   "source": [
    "* Dropping Columns\n",
    "* Dropping Rows\n",
    "* Various Parameter In Dropping functionalities\n",
    "* Handling Missing values by Mean, MEdian And Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f492eb5-d23c-490a-81e3-28f9b04f8003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#charge the table with missing values\n",
    "df_missing_value = spark_runing.read.csv('test2.csv', header = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ea75c48-3fcf-4a66-b520-0980b20d8fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|       34|  10|     38000|  null|\n",
      "|       36|null|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_missing_value.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f60eb-a757-47b9-8bd7-2dc908eb039f",
   "metadata": {},
   "source": [
    "# Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cf3549f-c6e4-4227-bbe3-46227e5b0dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#na = select all null values\n",
    "df_missing_value.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "211e093e-3e8a-4be0-adef-4343bededd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|       34|  10|     38000|  null|\n",
      "|       36|null|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping only valies that have all values null\n",
    "#any == all in how\n",
    "df_missing_value.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e14e0d5-b31b-4aee-a718-d94bfcf97d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|       34|  10|     38000|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#treshhold\n",
    "#only add using a number of null variables\n",
    "df_missing_value.na.drop(how=\"all\", thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bb2f5f1-b2c5-4dd1-bd2e-7acff3f8f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|       34| 10|     38000|  null|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#subset\n",
    "#drop any values from \n",
    "#subset = drop a values depending on a variable\n",
    "df_missing_value.na.drop(how=\"any\", subset=\"Experience\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe4009-c8c7-4cff-9d0b-11f14d1fd2f4",
   "metadata": {},
   "source": [
    "# filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99aa4de7-8252-4e05-9fcc-236e12eb712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filling_Va = df_missing_value.na.fill(0, subset=[\"salary\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17c3ceff-5a7e-45ad-83bc-ca783e5daa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh|  0|      null| 40000|\n",
      "|       34| 10|     38000|     0|\n",
      "|       36|  0|      null|     0|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filling_Va.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1353940-38a3-4b21-a253-ac9c0cb458cf",
   "metadata": {},
   "source": [
    "fill mising values using the\n",
    "* Mean \n",
    "* Median\n",
    "* Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f1dd5d3-9b6d-4411-b6c9-8b01da448edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation estimator for completing missing values, using the mean, median or mode\n",
    "from pyspark.ml.feature import Imputer\n",
    "addingvalues= Imputer(\n",
    "    #Whis its were im getting the imputs\n",
    "    inputCols=['age','Experience','Salary'],\n",
    "    #this allows to create a new column\n",
    "    outputCols= [\"{}_imputed\".format(c) \n",
    "        for c in ['age','Experience','Salary']]\n",
    "    #setStrategy() = this calculate the mean\n",
    "    ).setStrategy('mean')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5bc1dff7-0218-4b1b-8c6f-a4f54548f8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|null|      null| 40000|         24|              5432|         40000|\n",
      "|       34|  10|     38000|  null|         10|             38000|         24000|\n",
      "|       36|null|      null|  null|         24|              5432|         24000|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fit() = Fits a model to the input dataset with optional parameters.\n",
    "     #fit(): transformer learns something about the data\n",
    "#transform() = Concise syntax for chaining custom transformations.\n",
    "    #transform(): it uses what it learned to do the data transformation\n",
    "addingvalues.fit(df_missing_value).transform(df_missing_value).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535e193-c58e-4138-83ae-149d7d323fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4f5d928-fcd4-4753-a44c-39fa2781891a",
   "metadata": {},
   "source": [
    "* CountVectorizer\n",
    "    - fit learns the vocabulary\n",
    "    - transform creates a document-term matrix using the vocabulary\n",
    "* SimpleImputer\n",
    "    * fit learns the value to impute\n",
    "    * transform fills in missing entries using the imputation value\n",
    "* StandardScaler\n",
    "    * fit learns the mean and scale of each feature\n",
    "    * transform standardizes the features using the mean and scale\n",
    "* HashingVectorizer\n",
    "    * fit is not used, and thus it is known as a \"stateless\" transformer\n",
    "    * transform creates the document-term matrix using a hash of the token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aedcc4-6b0d-4fbf-8ce1-afd95bbeb1a1",
   "metadata": {},
   "source": [
    "# Diference between Fit() & Transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65d335-cfac-442e-ab68-d468d585f1e4",
   "metadata": {},
   "source": [
    "1.Fit(): Method calculates the parameters μ and σ and saves them as internal objects.\n",
    "\n",
    "2.Transform(): Method using these calculated parameters apply the transformation to a particular dataset.\n",
    "\n",
    "3.Fit_transform(): joins the fit() and transform() method for transformation of dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
